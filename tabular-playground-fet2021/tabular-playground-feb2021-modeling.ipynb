{"cells":[{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n!pip install optuna \n\n# category variable\ncategory = ['cat0', 'cat1', 'cat2', 'cat3', 'cat4', 'cat5', 'cat6', 'cat7', 'cat8', 'cat9']\n\n# continuous variable\ncontinuous = ['cont0', 'cont1', 'cont2', 'cont3', 'cont4', 'cont5', 'cont6', \n              'cont7', 'cont8', 'cont9', 'cont10', 'cont11', 'cont12', 'cont13']","execution_count":1,"outputs":[{"output_type":"stream","text":"Requirement already satisfied: optuna in /opt/conda/lib/python3.7/site-packages (2.4.0)\nRequirement already satisfied: colorlog in /opt/conda/lib/python3.7/site-packages (from optuna) (4.7.2)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from optuna) (20.8)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from optuna) (4.55.1)\nRequirement already satisfied: cliff in /opt/conda/lib/python3.7/site-packages (from optuna) (3.6.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from optuna) (1.19.5)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from optuna) (1.0.0)\nRequirement already satisfied: alembic in /opt/conda/lib/python3.7/site-packages (from optuna) (1.5.2)\nRequirement already satisfied: cmaes>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from optuna) (0.7.0)\nRequirement already satisfied: sqlalchemy>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from optuna) (1.3.22)\nRequirement already satisfied: scipy!=1.4.0 in /opt/conda/lib/python3.7/site-packages (from optuna) (1.4.1)\nRequirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->optuna) (2.4.7)\nRequirement already satisfied: python-editor>=0.3 in /opt/conda/lib/python3.7/site-packages (from alembic->optuna) (1.0.4)\nRequirement already satisfied: Mako in /opt/conda/lib/python3.7/site-packages (from alembic->optuna) (1.1.4)\nRequirement already satisfied: python-dateutil in /opt/conda/lib/python3.7/site-packages (from alembic->optuna) (2.8.1)\nRequirement already satisfied: pbr!=2.1.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from cliff->optuna) (5.5.1)\nRequirement already satisfied: cmd2!=0.8.3,>=0.8.0 in /opt/conda/lib/python3.7/site-packages (from cliff->optuna) (1.4.0)\nRequirement already satisfied: PyYAML>=3.12 in /opt/conda/lib/python3.7/site-packages (from cliff->optuna) (5.3.1)\nRequirement already satisfied: PrettyTable<0.8,>=0.7.2 in /opt/conda/lib/python3.7/site-packages (from cliff->optuna) (0.7.2)\nRequirement already satisfied: stevedore>=2.0.1 in /opt/conda/lib/python3.7/site-packages (from cliff->optuna) (3.3.0)\nRequirement already satisfied: wcwidth>=0.1.7 in /opt/conda/lib/python3.7/site-packages (from cmd2!=0.8.3,>=0.8.0->cliff->optuna) (0.2.5)\nRequirement already satisfied: importlib-metadata>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from cmd2!=0.8.3,>=0.8.0->cliff->optuna) (3.3.0)\nRequirement already satisfied: attrs>=16.3.0 in /opt/conda/lib/python3.7/site-packages (from cmd2!=0.8.3,>=0.8.0->cliff->optuna) (20.3.0)\nRequirement already satisfied: colorama>=0.3.7 in /opt/conda/lib/python3.7/site-packages (from cmd2!=0.8.3,>=0.8.0->cliff->optuna) (0.4.4)\nRequirement already satisfied: pyperclip>=1.6 in /opt/conda/lib/python3.7/site-packages (from cmd2!=0.8.3,>=0.8.0->cliff->optuna) (1.8.1)\nRequirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=1.6.0->cmd2!=0.8.3,>=0.8.0->cliff->optuna) (3.7.4.3)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=1.6.0->cmd2!=0.8.3,>=0.8.0->cliff->optuna) (3.4.0)\nRequirement already satisfied: MarkupSafe>=0.9.2 in /opt/conda/lib/python3.7/site-packages (from Mako->alembic->optuna) (1.1.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil->alembic->optuna) (1.15.0)\n\u001b[33mWARNING: You are using pip version 21.0; however, version 21.0.1 is available.\nYou should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n","name":"stdout"}]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"train_dataset = pd.read_csv('../input/tabular-playground-series-feb-2021/train.csv')\ntest_data = pd.read_csv('../input/tabular-playground-series-feb-2021/test.csv')\ndataset = pd.concat([train_dataset, test_data])\n#print(dataset.info())\n\n# あきらかな外れ値は削除\n# [166042]はあきらかな外れ値\ntarget_mean = dataset.describe().loc['mean', 'target']\ntarget_std = dataset.describe().loc['std', 'target']\nup = target_mean+4*target_std\nlow=target_mean-4*target_std\n\noutlier = dataset.loc[dataset['target'] < low, 'id'].values\n#outlier=[166042]\nfor x in outlier:\n    dataset = dataset.loc[dataset['id'] != x, :] \n\n# idとtargetは避難させておく\nid = dataset['id']\ntarget = dataset['target']\n# 相関が高いものだけを使用。この手法は有効だったと思うが、Catboostでのスコアは一定以上伸びなかった。\n \n    \n## ***** Feature Engineering - Create new columns ***** ##\n#for x in range(0,10):\n#    dataset[format(x, '03')] = dataset[('cont' + str(x))] * dataset[('cat' + str(x))]\n## **************************************************** ##    \n\n\n## ***** Encoding ***** ##\nfrom sklearn.preprocessing import LabelEncoder, RobustScaler, StandardScaler\nscaler = RobustScaler()\nencoder = LabelEncoder()\n\ncat_index = []\nfor col in dataset.columns:\n#    # カテゴリ変数はエンコーディングを適用\n    if dataset[col].dtype == object:\n        cat_index.append(dataset.columns.get_loc(col))\n        dataset[col] = encoder.fit_transform(dataset[col])\n\n#dataset[continuous] = scaler.fit_transform(dataset.loc[:,continuous].values)\n\n# targetのあるなしでtrainとtestを分割\ntrain = dataset.loc[dataset['target'].notnull(), :]\ntest  = dataset.loc[dataset['target'].isnull(), :]\n\nX = train.drop(columns=['id', 'target'])\ny = train['target']\nX_prediction = test.drop(columns=['id', 'target'])\nprediction_id = test.loc[:,'id']\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n\ndataset","execution_count":2,"outputs":[{"output_type":"execute_result","execution_count":2,"data":{"text/plain":"            id  cat0  cat1  cat2  cat3  cat4  cat5  cat6  cat7  cat8  ...  \\\n0            1     0     1     0     0     1     3     0     4     2  ...   \n1            2     1     0     0     0     1     1     0     4     0  ...   \n2            3     0     0     0     2     1     3     0     1     2  ...   \n3            4     0     0     0     2     1     3     0     4     6  ...   \n4            6     0     1     0     0     1     1     0     4     2  ...   \n...        ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  ...   \n199995  499987     0     0     0     2     1     3     0     4     6  ...   \n199996  499990     0     0     0     2     1     3     0     4     4  ...   \n199997  499991     0     0     0     2     1     3     0     4     2  ...   \n199998  499994     0     1     0     0     1     3     0     4     2  ...   \n199999  499995     0     1     0     2     1     2     0     4     6  ...   \n\n           cont5     cont6     cont7     cont8     cont9    cont10    cont11  \\\n0       0.881122  0.421650  0.741413  0.895799  0.802461  0.724417  0.701915   \n1       0.440011  0.346230  0.278495  0.593413  0.546056  0.613252  0.741289   \n2       0.914155  0.369602  0.832564  0.865620  0.825251  0.264104  0.695561   \n3       0.934138  0.578930  0.407313  0.868099  0.794402  0.494269  0.698125   \n4       0.382600  0.705940  0.325193  0.440967  0.462146  0.724447  0.683073   \n...          ...       ...       ...       ...       ...       ...       ...   \n199995  0.963678  0.240482  0.686462  0.915165  0.848878  0.459598  0.590327   \n199996  0.232072  0.363421  0.694092  0.137002  0.319465  0.364527  0.388908   \n199997  0.432927  0.811876  0.328398  0.496017  0.538779  0.466338  0.643869   \n199998  0.837712  0.680886  0.534439  0.501588  0.809053  0.631704  0.766426   \n199999  0.792263  0.409425  0.285033  0.594721  0.824892  0.479586  0.683065   \n\n          cont12    cont13    target  \n0       0.877618  0.719903  6.994023  \n1       0.326679  0.808464  8.071256  \n2       0.869133  0.828352  5.760456  \n3       0.809799  0.614766  7.806457  \n4       0.343457  0.297743  6.868974  \n...          ...       ...       ...  \n199995  0.864873  0.425258       NaN  \n199996  0.664357  0.224215       NaN  \n199997  0.749590  0.457702       NaN  \n199998  0.937139  0.796304       NaN  \n199999  0.721518  0.722690       NaN  \n\n[499976 rows x 26 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>cat0</th>\n      <th>cat1</th>\n      <th>cat2</th>\n      <th>cat3</th>\n      <th>cat4</th>\n      <th>cat5</th>\n      <th>cat6</th>\n      <th>cat7</th>\n      <th>cat8</th>\n      <th>...</th>\n      <th>cont5</th>\n      <th>cont6</th>\n      <th>cont7</th>\n      <th>cont8</th>\n      <th>cont9</th>\n      <th>cont10</th>\n      <th>cont11</th>\n      <th>cont12</th>\n      <th>cont13</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>3</td>\n      <td>0</td>\n      <td>4</td>\n      <td>2</td>\n      <td>...</td>\n      <td>0.881122</td>\n      <td>0.421650</td>\n      <td>0.741413</td>\n      <td>0.895799</td>\n      <td>0.802461</td>\n      <td>0.724417</td>\n      <td>0.701915</td>\n      <td>0.877618</td>\n      <td>0.719903</td>\n      <td>6.994023</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>4</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0.440011</td>\n      <td>0.346230</td>\n      <td>0.278495</td>\n      <td>0.593413</td>\n      <td>0.546056</td>\n      <td>0.613252</td>\n      <td>0.741289</td>\n      <td>0.326679</td>\n      <td>0.808464</td>\n      <td>8.071256</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>1</td>\n      <td>3</td>\n      <td>0</td>\n      <td>1</td>\n      <td>2</td>\n      <td>...</td>\n      <td>0.914155</td>\n      <td>0.369602</td>\n      <td>0.832564</td>\n      <td>0.865620</td>\n      <td>0.825251</td>\n      <td>0.264104</td>\n      <td>0.695561</td>\n      <td>0.869133</td>\n      <td>0.828352</td>\n      <td>5.760456</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>1</td>\n      <td>3</td>\n      <td>0</td>\n      <td>4</td>\n      <td>6</td>\n      <td>...</td>\n      <td>0.934138</td>\n      <td>0.578930</td>\n      <td>0.407313</td>\n      <td>0.868099</td>\n      <td>0.794402</td>\n      <td>0.494269</td>\n      <td>0.698125</td>\n      <td>0.809799</td>\n      <td>0.614766</td>\n      <td>7.806457</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>6</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>4</td>\n      <td>2</td>\n      <td>...</td>\n      <td>0.382600</td>\n      <td>0.705940</td>\n      <td>0.325193</td>\n      <td>0.440967</td>\n      <td>0.462146</td>\n      <td>0.724447</td>\n      <td>0.683073</td>\n      <td>0.343457</td>\n      <td>0.297743</td>\n      <td>6.868974</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>199995</th>\n      <td>499987</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>1</td>\n      <td>3</td>\n      <td>0</td>\n      <td>4</td>\n      <td>6</td>\n      <td>...</td>\n      <td>0.963678</td>\n      <td>0.240482</td>\n      <td>0.686462</td>\n      <td>0.915165</td>\n      <td>0.848878</td>\n      <td>0.459598</td>\n      <td>0.590327</td>\n      <td>0.864873</td>\n      <td>0.425258</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>199996</th>\n      <td>499990</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>1</td>\n      <td>3</td>\n      <td>0</td>\n      <td>4</td>\n      <td>4</td>\n      <td>...</td>\n      <td>0.232072</td>\n      <td>0.363421</td>\n      <td>0.694092</td>\n      <td>0.137002</td>\n      <td>0.319465</td>\n      <td>0.364527</td>\n      <td>0.388908</td>\n      <td>0.664357</td>\n      <td>0.224215</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>199997</th>\n      <td>499991</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>1</td>\n      <td>3</td>\n      <td>0</td>\n      <td>4</td>\n      <td>2</td>\n      <td>...</td>\n      <td>0.432927</td>\n      <td>0.811876</td>\n      <td>0.328398</td>\n      <td>0.496017</td>\n      <td>0.538779</td>\n      <td>0.466338</td>\n      <td>0.643869</td>\n      <td>0.749590</td>\n      <td>0.457702</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>199998</th>\n      <td>499994</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>3</td>\n      <td>0</td>\n      <td>4</td>\n      <td>2</td>\n      <td>...</td>\n      <td>0.837712</td>\n      <td>0.680886</td>\n      <td>0.534439</td>\n      <td>0.501588</td>\n      <td>0.809053</td>\n      <td>0.631704</td>\n      <td>0.766426</td>\n      <td>0.937139</td>\n      <td>0.796304</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>199999</th>\n      <td>499995</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>2</td>\n      <td>1</td>\n      <td>2</td>\n      <td>0</td>\n      <td>4</td>\n      <td>6</td>\n      <td>...</td>\n      <td>0.792263</td>\n      <td>0.409425</td>\n      <td>0.285033</td>\n      <td>0.594721</td>\n      <td>0.824892</td>\n      <td>0.479586</td>\n      <td>0.683065</td>\n      <td>0.721518</td>\n      <td>0.722690</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n<p>499976 rows × 26 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nimport statistics as stat\nimport lightgbm as lgb\n\ndef kfold_processing(models, X, y):\n    kf = KFold(n_splits=5,random_state=48,shuffle=True)\n    rmse=[]  # list contains rmse for each fold\n    n=0\n    preds=0\n    \n    for model in models:\n        # kf.splitはindexを返すことに注意。データ自体じゃないよ!\n        for train, test in kf.split(X, y):\n            x_tr = X.iloc[train, :]\n            x_te = X.iloc[test,  :]\n            y_tr = y.iloc[train]\n            y_te = y.iloc[test]\n            model.fit(x_tr,y_tr,eval_set=[(x_te,y_te)],early_stopping_rounds=100,verbose=False)\n            rmse_metric = mean_squared_error(y_te, model.predict(x_te), squared=False)\n            rmse.append(rmse_metric)\n            preds+=model.predict(X_prediction)\n            print(n+1,rmse[n])\n            n+=1\n\n        print(f'RMSE mean = {stat.mean(rmse)}')\n        print(f'RMSE = {rmse}')\n    return preds","execution_count":14,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# sklearn.ensemble.StackingRegressor — scikit-learn 0.24.1 documentation \n# https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.StackingRegressor.html\n\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom sklearn.ensemble import StackingRegressor\nbest_lgb_params = {'objective': 'regression', 'num_leaves': 87, 'max_depth': 9, 'learning_rate': 0.009582240516938432, 'n_estimators': 2553, \n                   'reg_alpha': 0.037580598353005736, 'reg_lambda': 0.02686007922451687, 'colsample_bytree': 0.5534542046480458}\n\nbest_xgb_params = {'objective': 'reg:squarederror', 'eval_metric': 'rmse', 'tree_method': 'gpu_hist','eta': 0.6967322281466614, \n                   'gamma': 0.0036942646535044962, 'max_depth': 6, 'sub_sample': 0.553569633282626, \n                   'colsample_bytree': 0.554663669927102, 'lambda': 0.001701369247469206, 'alpha': 0.009630419498745284, \n                   'learning_rate': 0.03446986865236482, 'n_estimators': 603}      \n\nlgb_model = lgb.LGBMRegressor(**best_lgb_params)\nxgb_model = xgb.XGBRegressor(**best_xgb_params)\nkf = KFold(n_splits=5,random_state=48,shuffle=True)\n\nmodels = [lgb_model]\n\ny_pred = kfold_processing(models=models, X=X, y=y)\n\n# StackingRegressorのトライ\n'''\nestimators = [\n    ('lgb', lgb_model),\n    ('xgb_model', xgb_model)\n]\n\nstreg = StackingRegressor(estimators=estimators, cv=kf)\nstreg.fit(X,y)\n'''","execution_count":13,"outputs":[{"output_type":"stream","text":"1 0.8445690179759722\n2 0.8423412780092387\n3 0.8423321367289558\n4 0.8422793257727865\n5 0.8440768228325857\nRMSE mean = 0.8431197162639078\nRMSE = [0.8445690179759722, 0.8423412780092387, 0.8423321367289558, 0.8422793257727865, 0.8440768228325857]\n","name":"stdout"},{"output_type":"execute_result","execution_count":13,"data":{"text/plain":"\"\\nestimators = [\\n    ('lgb', lgb_model),\\n    ('xgb_model', xgb_model)\\n]\\n\\nstreg = StackingRegressor(estimators=estimators, cv=kf)\\nstreg.fit(X,y)\\n\""},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"### \"num_iteration=model.best_iteration_\"があるとき\n> \n1 0.8445690179759722  \n2 0.8423412780092387  \n3 0.8423321367289558  \n4 0.8422793257727865  \n5 0.8440768228325857  \nRMSE mean = 0.8431197162639078  \nRMSE = [0.8445690179759722, 0.8423412780092387, 0.8423321367289558, 0.8422793257727865, 0.8440768228325857]\n\n### \"num_iteration=model.best_iteration_\"がないとき\n>\n1 0.8445690179759722  \n2 0.8423412780092387  \n3 0.8423321367289558  \n4 0.8422793257727865  \n5 0.8440768228325857  \nRMSE mean = 0.8431197162639078  \nRMSE = [0.8445690179759722, 0.8423412780092387, 0.8423321367289558, 0.8422793257727865, 0.8440768228325857]\n"},{"metadata":{},"cell_type":"markdown","source":"### Optuna Tuning\n[lightgbm.LGBMRegressor — LightGBM 3.1.1.99 documentation](https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMRegressor.html)  \n[XGBoostパラメータのまとめとランダムサーチ実装 - Qiita](https://qiita.com/FJyusk56/items/0649f4362587261bd57a)  \n[XGBoost Parameters — xgboost 1.4.0-SNAPSHOT documentation](https://xgboost.readthedocs.io/en/latest/parameter.html)  \n[CatBoostRegressor - CatBoost. Documentation](https://catboost.ai/docs/concepts/python-reference_catboostregressor.html)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lightgbm Optuna\n\nimport lightgbm as lgb\nfrom sklearn.metrics import r2_score, mean_squared_error\nimport optuna\n\ntrains = lgb.Dataset(X_train, y_train)\ntests = lgb.Dataset(X_test, y_test)\n\ndef objective(trial):\n    params = {\n        'objective': 'regression',\n        'num_leaves': trial.suggest_int('num_leaves', 5, 200),\n        'max_depth': trial.suggest_int('max_depth', 4, 10),\n        'learning_rate': trial.suggest_loguniform('learning_rate', 0.001, 0.1),\n        'n_estimators': trial.suggest_int('n_estimators', 100, 3000),\n        'reg_alpha': trial.suggest_loguniform('reg_alpha', 0.001, 0.1),\n        'reg_lambda': trial.suggest_loguniform('reg_lambda', 0.001, 0.1),\n        'colsample_bytree': trial.suggest_loguniform('colsample_bytree', 0.5, 1),\n        'random_state': 42\n    }  \n    \n    \n    lgb_model = lgb.LGBMRegressor(**params)\n    lgb_model.fit(X=X_train, y=y_train, eval_set = [(X_test, y_test)], eval_metric='rmse',  early_stopping_rounds=10, verbose=False)\n    y_pred = lgb_model.predict(X_test, num_iteration=lgb_model.best_iteration_)\n    \n    return mean_squared_error(y_test, y_pred)\n\nstudy = optuna.create_study()\nstudy.optimize(objective, n_trials=50, n_jobs=-1)\nlgb_best = study.best_params\nprint(lgb_best)","execution_count":7,"outputs":[{"output_type":"stream","text":"\u001b[32m[I 2021-03-01 02:04:13,257]\u001b[0m A new study created in memory with name: no-name-d32588ad-ed2c-4d59-a525-b2020472a287\u001b[0m\n\u001b[32m[I 2021-03-01 02:05:27,558]\u001b[0m Trial 0 finished with value: 0.713292361998873 and parameters: {'num_leaves': 85, 'max_depth': 9, 'learning_rate': 0.004856916551979603, 'n_estimators': 2834, 'reg_alpha': 0.0021415442776480803, 'reg_lambda': 0.005469639847921993, 'colsample_bytree': 0.91094607014517}. Best is trial 0 with value: 0.713292361998873.\u001b[0m\n","name":"stderr"},{"output_type":"stream","text":"{'num_leaves': 85, 'max_depth': 9, 'learning_rate': 0.004856916551979603, 'n_estimators': 2834, 'reg_alpha': 0.0021415442776480803, 'reg_lambda': 0.005469639847921993, 'colsample_bytree': 0.91094607014517}\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# XGBoost Optuna\n\nimport xgboost as xgb\nfrom sklearn.metrics import r2_score, mean_squared_error\nimport optuna\n\n# XGBoost Parameters — xgboost 1.4.0-SNAPSHOT documentation https://xgboost.readthedocs.io/en/latest/parameter.html\n\ndef objective(trial):\n    params = {\n        'objective': 'reg:squarederror',\n        'eval_metric': 'rmse',\n        'tree_method': 'gpu_hist',\n        'eta': trial.suggest_loguniform('eta', 0.1, 1.0),\n        'gamma': trial.suggest_loguniform('gamma', 0.001, 5.),\n        'max_depth': trial.suggest_int('max_depth', 4, 10),\n        'sub_sample': trial.suggest_loguniform('sub_sample', 0.5, 1.0),\n        'colsample_bytree': trial.suggest_loguniform('colsample_bytree', 0.5, 1.0),\n        'lambda': trial.suggest_loguniform('lambda', 0.001, 0.01),\n        'alpha': trial.suggest_loguniform('alpha', 0.001, 0.01),\n        'learning_rate': trial.suggest_loguniform('learning_rate', 0.0001, 0.1),\n        'n_estimators': trial.suggest_int('n_estimators', 100, 1000)\n    }  \n    \n    xgb_model = xgb.XGBRegressor(**params)\n    xgb_model.fit(X_train, y_train, eval_set = [(X_test, y_test)],  early_stopping_rounds=10)\n    y_pred = xgb_model.predict(X_test)\n    \n    return np.sqrt(mean_squared_error(y_test, y_pred))\n\nstudy = optuna.create_study()\nstudy.optimize(objective, n_trials=50, n_jobs=-1)\nxgb_best = study.best_params\nprint(xgb_best)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## CatBoost Optuna\n\nfrom catboost import CatBoostRegressor\nfrom catboost import Pool\nfrom sklearn.metrics import r2_score, mean_squared_error\nimport optuna\n\ncat_train = Pool(X_train, y_train, cat_features=category)\ncat_test = Pool(X_test, y_test, cat_features=category)\n\ndef objective(trial):\n    params = {\n        'iterations': trial.suggest_int('iterations', 100, 3000),\n        'depth': trial.suggest_int('depth', 4, 10),\n        'learning_rate': trial.suggest_loguniform('learning_rate', 0.001, 0.1),\n        'l2_leaf_reg': trial.suggest_int('l2_leaf', 1,10),\n        'eval_metric': 'RMSE',\n        'random_strength': trial.suggest_int('random_strength', 0, 100),\n        'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.01, 100.00),\n        'od_type': trial.suggest_categorical('od_type', ['IncToDec', 'Iter']),\n        'od_wait': trial.suggest_int('od_wait', 10, 50),\n        'use_best_model': True\n    }  \n    \n    model = CatBoostRegressor(**params)\n    model.fit(cat_train, eval_set=cat_test, early_stopping_rounds=10, verbose=0)\n    y_pred = model.predict(cat_test)\n    \n    return np.sqrt(mean_squared_error(y_test, y_pred))\n\nstudy = optuna.create_study()\nstudy.optimize(objective, n_trials=100, n_jobs=-1)\ncat_best = study.best_params\nprint(cat_best)\nprint(f'Best Value: {study.best_value}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## prediction後に結果をファイルへ出力\n\n#y_pred=regressor.predict(X_prediction)\nprint(y_pred/10)\noutput = pd.DataFrame({'id': prediction_id, 'target': y_pred/10})\noutput.to_csv('submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## (参考) Lightgbm Optuna\n\nimport optuna.integration.lightgbm as lgb\n\ntrains = lgb.Dataset(X_train, y_train)\ntests = lgb.Dataset(X_test, y_test)\n\nparams = {'objective': 'mean_squared_error',\n         'metric': 'rmse'}\n\nlgb_model = lgb.train(params, trains, valid_sets=tests, early_stopping_rounds=10)\nbest_params = lgb_model.params\n\n\nimport lightgbm as lgb\n\nlgb_model = lgb.train(best_params, trains, valid_sets=tests)\ny_pred = lgb_model.predict(X_test)\n\nfrom sklearn.metrics import mean_squared_error\nprint(mean_squared_error(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"## (参考) TensorFlow Regression\nimport tensorflow as tf\nfrom tensorflow import keras\n\ndef rmse(y_true, y_pred):\n    return tf.sqrt(tf.losses.mean_squared_error(y_true, y_pred))\n\n### ***********************************************************###\nmodel = tf.keras.Sequential()\n\n#model.add(tf.keras.layers.Dense(1024, activation='relu', input_shape=(X.shape[1],)))\n#model.add(tf.keras.layers.LeakyReLU())\n#model.add(tf.keras.layers.BatchNormalization())\n#model.add(tf.keras.layers.Dropout(0.5))\n\n#model.add(tf.keras.layers.Dense(512, activation='relu'))\nmodel.add(tf.keras.layers.Dense(64, kernel_regularizer=keras.regularizers.l2(0.001), activation='relu',input_shape=(X.shape[1],)))\nmodel.add(tf.keras.layers.LeakyReLU())\nmodel.add(tf.keras.layers.BatchNormalization())\nmodel.add(tf.keras.layers.Dropout(0.4))\n\n#model.add(tf.keras.layers.Dense(512, activation='relu'))\nmodel.add(tf.keras.layers.Dense(64, kernel_regularizer=keras.regularizers.l2(0.001), activation='relu'))\nmodel.add(tf.keras.layers.LeakyReLU())\nmodel.add(tf.keras.layers.BatchNormalization())\nmodel.add(tf.keras.layers.Dropout(0.4))\n\n#model.add(tf.keras.layers.Dense(units=256, activation='relu'))\nmodel.add(tf.keras.layers.Dense(32, kernel_regularizer=keras.regularizers.l2(0.001), activation='relu'))\nmodel.add(tf.keras.layers.LeakyReLU())\nmodel.add(tf.keras.layers.BatchNormalization())\nmodel.add(tf.keras.layers.Dropout(0.3))\n\n#model.add(tf.keras.layers.Dense(units=256, activation='relu'))\nmodel.add(tf.keras.layers.Dense(32, kernel_regularizer=keras.regularizers.l2(0.001), activation='relu'))\nmodel.add(tf.keras.layers.LeakyReLU())\nmodel.add(tf.keras.layers.BatchNormalization())\nmodel.add(tf.keras.layers.Dropout(0.3))\n\n#model.add(tf.keras.layers.Dense(units=128, activation='relu'))\nmodel.add(tf.keras.layers.Dense(16, kernel_regularizer=keras.regularizers.l2(0.001), activation='relu'))\nmodel.add(tf.keras.layers.LeakyReLU())\n\nmodel.add(tf.keras.layers.Dense(units=1, activation='linear'))\n### ***********************************************************###\n\noptimizer = tf.keras.optimizers.Adam(lr=0.005, decay=5e-4)\nmodel.compile(optimizer = optimizer, loss = 'mae', metrics = ['mse', 'mae'])\n\n#checkpoint_name = 'Model/{epoch:03d}-{val_loss:.5f}.hdf5'\ncheckpoint_name = 'DNN_BestModel.hdf5'\ncheckpoint = tf.keras.callbacks.ModelCheckpoint(checkpoint_name, monitor='val_loss', verbose=1, save_best_only=True, mode='auto')\ncallback_list = [checkpoint]\n\nhistory = model.fit(X_train, y_train, validation_split=0.2, epochs = 500, batch_size = 1024,\n                    validation_data=(X_test, y_test), callbacks=callback_list)\ny_pred = model.predict(X_test).reshape(-1)\nRMSE = rmse(y_test, y_pred)\nprint(f'RMSE = {RMSE}')\n\nfig = plt.figure(figsize=(8,5))\nax = fig.add_subplot(111)\nax.set_ylim(0.6, 0.75)\n\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.legend(['Train', 'Test'], loc='upper right')\nplt.savefig(fname='1024 neurons enable LearningRate.png')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}