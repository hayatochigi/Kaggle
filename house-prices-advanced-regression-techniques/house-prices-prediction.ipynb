{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 81)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Raw dataset shape: 1460 * 81\n"
     ]
    }
   ],
   "source": [
    "# trainデータを取得、NaN値などの状況を確認\n",
    "dataset = pd.read_csv('data/train.csv', sep=',')\n",
    "test_dataset = pd.read_csv('data/test.csv', sep=',')\n",
    "dataset_header = dataset.columns\n",
    "print(f'Raw dataset shape: {dataset.shape[0]} * {dataset.shape[1]}')\n",
    "# NaNチェック\n",
    "# datasetから一列ずつ取り出して、nullが含まれている場合はそのカラム名を返す\n",
    "def print_NaN_information(dataset):\n",
    "    for data in dataset:\n",
    "        numof_null = dataset[data].isnull().sum()\n",
    "        if numof_null > 0:\n",
    "            ratio = (numof_null / dataset.shape[0]) * 100\n",
    "            print(f'NaN in {data}:{numof_null} \\tNaN, ratio is\\t{ratio:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nan_imputer(dataset):\n",
    "    # NaNデータの処理\n",
    "    # 以下のカラムリストについては、NAのものは\"None\"に置き換えることで欠損値を補完する\n",
    "    convert_nan_to_other_list = ['Alley', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2',\n",
    "                                'FireplaceQu', 'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond',\n",
    "                                'PoolQC', 'Fence', 'MiscFeature']\n",
    "    convert_nan_to_other_list = dict.fromkeys(convert_nan_to_other_list, 'None')\n",
    "    dataset = dataset.fillna(convert_nan_to_other_list)\n",
    "\n",
    "    # ******************************************************************************#\n",
    "    # Basementの処理。BsmtQual, BsmtCond, BsmtExposure, BsmtFinType1のいずれかが'None'なら、\n",
    "    # BsmtFinType2はNoneに、BsmtFinSF1とBsmtFinSF2, BsmtUnfSF, TotalBsmtSFは0となる\n",
    "    # どれかから地下室がない情報が得られていて、もし他のデータに齟齬や欠損があれば、その情報で補完\n",
    "\n",
    "    # BsmtFinType1がUnfなら、BsmtFinSF1は0となる\n",
    "    # BsmtFinType2がUnfなら、BsmtFinSF2は0となる\n",
    "    # ベースメントの建設がUnfなら、完成エリアの面積は0で補完\n",
    "    basement_condition = (dataset['BsmtQual'] == 'None')| (dataset['BsmtCond'] == 'None') | (dataset['BsmtExposure'] == 'None') | (dataset['BsmtFinType1'] == 'None')\n",
    "    basement_target = ['BsmtFinSF1', 'BsmtFinType2', 'BsmtUnfSF', 'TotalBsmtSF']\n",
    "    dataset.loc[basement_condition, basement_target] = 0\n",
    "    dataset.loc[basement_condition, ['BsmtFinType2']] = 'None'\n",
    "\n",
    "    dataset.loc[(dataset['BsmtFinType1'] == 'Unf'), ['BsmtFinSF1']] = 0\n",
    "    dataset.loc[(dataset['BsmtFinType2'] == 'Unf'), ['BsmtFinSF2']] = 0\n",
    "    # ******************************************************************************#\n",
    "\n",
    "    # LotFrontageのNAは、接続無しとして0で置換\n",
    "    dataset[['LotFrontage']] = dataset['LotFrontage'].fillna(0)\n",
    "\n",
    "    # ******************************************************************************#\n",
    "    # Garageの処理。GarageTypeがNAの場合は製造年は最古 - 100年とする\n",
    "    # nanとの比較を行うときに、 dataset['aaa'] == np.nan としても、正しくnanを検出できないので避けること\n",
    "    # dataset.loc[(dataset['GarageType'].isnull() | (dataset['GarageType'] == 'None')), 'GarageYrBlt'] = dataset['GarageYrBlt'].dropna().value_counts().idxmin() - 100\n",
    "    # ******************************************************************************#\n",
    "\n",
    "    # ******************************************************************************#\n",
    "    # その他の欠損値処理\n",
    "    # いずれも名義尺度のため、NaNについては最頻値で補完してみる\n",
    "    dataset.loc[dataset['MasVnrType'].isnull(), 'MasVnrType'] = dataset['MasVnrType'].dropna().value_counts().idxmax()\n",
    "    dataset.loc[dataset['MasVnrArea'].isnull(), 'MasVnrArea'] = dataset['MasVnrArea'].dropna().value_counts().idxmax()\n",
    "    dataset.loc[dataset['Electrical'].isnull(), 'Electrical'] = dataset['Electrical'].dropna().value_counts().idxmax()\n",
    "    # ******************************************************************************#\n",
    "\n",
    "    print_NaN_information(dataset)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "dataset = nan_imputer(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Data Scalling\n",
    "# 数値尺度のカラム\n",
    "num_cols = ['LotFrontage', 'LotArea', 'YearBuilt', 'YearRemodAdd', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF',\n",
    "                '1stFlrSF', '2ndFlrSF', 'LowQualFinSF', 'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath', 'TotRmsAbvGrd', \n",
    "                'Fireplaces', 'GarageCars', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch','ScreenPorch',\n",
    "                'PoolArea', 'MiscVal']\n",
    "# 名義尺度のカラム。Idはいずれでもないのでpreprocessingの対象外。SalePriceはターゲットのため対象外。\n",
    "nominal_cols = [i for i in dataset.columns if i not in np.append(num_cols, ['Id', 'SalePrice'])]\n",
    "\n",
    "def data_preprocessing(dataset):\n",
    "\n",
    "    # ******************************************************************************#\n",
    "    # Yearの処理\n",
    "    this_year = 2021\n",
    "    #dataset['GarageYrBlt'] = this_year - dataset['GarageYrBlt']\n",
    "    #dataset['YrSold'] = this_year - dataset['YrSold']\n",
    "    # ******************************************************************************#\n",
    "\n",
    "    # ******************************************************************************#\n",
    "    # 正規分布に従っていないものはlogを取って\n",
    "    log_convert_required = ['LotFrontage', 'LotArea', '1stFlrSF', 'GrLivArea']\n",
    "    for x in log_convert_required:\n",
    "        # 0はlogを取ると'-Inf'となるのでclipで回避\n",
    "        dataset[x] = np.log(np.clip(dataset[x], a_min=1e-323, a_max=1e+10))\n",
    "    # ******************************************************************************#\n",
    "    # 数値尺度たちは外れ値も考慮してZ-Scoreでスケーリング\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    dataset[num_cols] = scaler.fit_transform(dataset.loc[:,num_cols].values)\n",
    "    # 名義尺度たちはOne-Hotエンコーディング。他の値で残りの1つはわかるので、drop_first=True\n",
    "    #dataset = pd.get_dummies(dataset, columns=nominal_cols, drop_first=True)\n",
    "    return dataset\n",
    "\n",
    "dataset = data_preprocessing(dataset)\n",
    "dataset.to_csv('preprocessed.csv')\n",
    "print_NaN_information(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Selection\n",
    "def get_train_test_data(dataset):\n",
    "    features = ['LotFrontage', 'LotArea', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'GrLivArea', 'GarageArea']\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    #X = dataset.loc[:,features].values\n",
    "    X = dataset.drop(['SalePrice', 'Id'], axis=1)\n",
    "    Id = dataset['Id']\n",
    "    if 'SalePrice' in dataset:\n",
    "        y = np.log(dataset['SalePrice'].values)\n",
    "    else:\n",
    "        y = np.nan\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test, Id\n",
    "\n",
    "X_train, X_test, y_train, y_test, Id = get_train_test_data(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "                        Results: Ordinary least squares\n===============================================================================\nModel:                  OLS              Adj. R-squared (uncentered): -0.005   \nDependent Variable:     y                AIC:                         9137.4720\nDate:                   2021-01-26 16:11 BIC:                         9172.9133\nNo. Observations:       1168             Log-Likelihood:              -4561.7  \nDf Model:               7                F-statistic:                 0.2496   \nDf Residuals:           1161             Prob (F-statistic):          0.972    \nR-squared (uncentered): 0.002            Scale:                       145.38   \n-------------------------------------------------------------------------------------\n          Coef.        Std.Err.          t          P>|t|         [0.025       0.975]\n-------------------------------------------------------------------------------------\nx1       -0.1326         0.3534       -0.3752       0.7076       -0.8259       0.5608\nx2        0.0298         0.4118        0.0724       0.9423       -0.7782       0.8378\nx3       -0.1334         0.6135       -0.2174       0.8279       -1.3370       1.0703\nx4       -0.0660         1.2122       -0.0544       0.9566       -2.4443       2.3124\nx5       -0.1659         1.2731       -0.1303       0.8963       -2.6638       2.3320\nx6        0.5329         1.4719        0.3621       0.7174       -2.3550       3.4208\nx7        0.1568         0.4450        0.3525       0.7246       -0.7163       1.0299\n-------------------------------------------------------------------------------\nOmnibus:                  75.115            Durbin-Watson:               0.001 \nProb(Omnibus):            0.000             Jarque-Bera (JB):            92.671\nSkew:                     -0.600            Prob(JB):                    0.000 \nKurtosis:                 3.680             Condition No.:               10    \n===============================================================================\n\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "model = sm.OLS(y_train,X_train)\n",
    "fit = model.fit()\n",
    "print(fit.summary2())\n",
    "#print(fit.summary2().tables[0])\n",
    "#p_values = fit.summary2().tables[1]['P>|t|']\n",
    "#print(p_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "R2 Score(train) is 0.96176\nR2 Score(test) is -1020489913696940544.00000\n"
     ]
    }
   ],
   "source": [
    "# (Multiple) Linear Regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(X_train,y_train)\n",
    "\n",
    "y_pred = regressor.predict(X_test)\n",
    "y_train_pred = regressor.predict(X_train)\n",
    "\n",
    "# trainデータとtestデータそれぞれのR2スコアを表示する関数\n",
    "def print_r2_score(y_train, y_test, y_train_pred, y_pred):\n",
    "    from sklearn.metrics import r2_score\n",
    "    print(f'R2 Score(train) is {r2_score(y_train, y_train_pred):.5f}')\n",
    "    print(f'R2 Score(test) is {r2_score(y_test, y_pred):.5f}')\n",
    "\n",
    "print_r2_score(y_train, y_test, y_train_pred, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "R2 Score(train) is 0.76559\nR2 Score(test) is 0.73769\n"
     ]
    }
   ],
   "source": [
    "# Polymial Regression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "polynomial_instance = PolynomialFeatures(degree = 2)\n",
    "X_poly_train = polynomial_instance.fit_transform(X_train)\n",
    "X_poly_test = polynomial_instance.fit_transform(X_test)\n",
    "\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(X_poly_train, y_train)\n",
    "y_train_pred = regressor.predict(X_poly_train)\n",
    "y_pred = regressor.predict(X_poly_test)\n",
    "\n",
    "print_r2_score(y_train, y_test, y_train_pred, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "R2 Score(train) is 0.73139\nR2 Score(test) is 0.52200\n"
     ]
    }
   ],
   "source": [
    "# SVR\n",
    "from sklearn.svm import SVR\n",
    "svr_regressor = SVR(kernel='linear')\n",
    "svr_regressor.fit(X_train, y_train)\n",
    "y_pred = svr_regressor.predict(X_test)\n",
    "y_train_pred = svr_regressor.predict(X_train)\n",
    "\n",
    "print_r2_score(y_train, y_test, y_train_pred, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "R2 Score(train) is 0.81207\nR2 Score(test) is 0.74288\n"
     ]
    }
   ],
   "source": [
    "# SVR (rbf)\n",
    "from sklearn.svm import SVR\n",
    "svr_regressor = SVR(kernel='rbf')\n",
    "svr_regressor.fit(X_train, y_train)\n",
    "y_pred = svr_regressor.predict(X_test)\n",
    "y_train_pred = svr_regressor.predict(X_train)\n",
    "\n",
    "print_r2_score(y_train, y_test, y_train_pred, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "R2 Score(train) is 0.98011\nR2 Score(test) is 0.86764\n"
     ]
    }
   ],
   "source": [
    "# Random Forest\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rndm_regressor = RandomForestRegressor(n_estimators = 100, random_state=0)\n",
    "rndm_regressor.fit(X_train, y_train)\n",
    "y_pred = rndm_regressor.predict(X_test)\n",
    "y_train_pred = rndm_regressor.predict(X_train)\n",
    "\n",
    "print_r2_score(y_train, y_test, y_train_pred, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-d3f8b0c8fdd1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m ]\n\u001b[0;32m     12\u001b[0m \u001b[0mgrid_search\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mRandomForestRegressor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrid_parameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'r2'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mgrid_search\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[0mgrid_search\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\ML-env\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     70\u001b[0m                           FutureWarning)\n\u001b[0;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\ML-env\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    734\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    735\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 736\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    737\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    738\u001b[0m         \u001b[1;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\ML-env\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1186\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1187\u001b[0m         \u001b[1;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1188\u001b[1;33m         \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1189\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\ML-env\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[1;34m(candidate_params)\u001b[0m\n\u001b[0;32m    713\u001b[0m                                \u001b[1;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    714\u001b[0m                                in product(candidate_params,\n\u001b[1;32m--> 715\u001b[1;33m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[0;32m    716\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    717\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\ML-env\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1052\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1054\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1055\u001b[0m             \u001b[1;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1056\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\ML-env\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    931\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    932\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'supports_timeout'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 933\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    934\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    935\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\ML-env\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[1;34m(future, timeout)\u001b[0m\n\u001b[0;32m    540\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[0;32m    541\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 542\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    543\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mCfTimeoutError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\ML-env\\lib\\concurrent\\futures\\_base.py\u001b[0m in \u001b[0;36mresult\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    425\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    426\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 427\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    428\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    429\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\ML-env\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    293\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m    \u001b[1;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    294\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 295\u001b[1;33m                 \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    297\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# grid search\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "grid_parameters = [\n",
    "    {'n_estimators': [1, 2, 5, 10, 100, 1000], \n",
    "    'criterion': ['mse', 'mae'],\n",
    "    'min_samples_split': [1, 2, 5, 10, 20],\n",
    "    'min_samples_leaf': [1, 2, 5, 10, 20],\n",
    "    'bootstrap': [True, False],\n",
    "    'max_features': ['auto', 'sqrt', 'log2']\n",
    "    }\n",
    "]\n",
    "grid_search = GridSearchCV(RandomForestRegressor(), grid_parameters, cv=5, scoring='r2', n_jobs = -1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "fillna() got an unexpected keyword argument 'subset'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-281-83bdbe6f6db9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mtest_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data/test.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#test_dataset = NaN_check(test_dataset)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mtest_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNaN_processing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mX_sub\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_sub\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_preprocessing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'predict'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-c35e88a8bfbb>\u001b[0m in \u001b[0;36mNaN_processing\u001b[1;34m(dataset)\u001b[0m\n\u001b[0;32m      5\u001b[0m                                 \u001b[1;34m'FireplaceQu'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'GarageType'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'GarageFinish'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'GarageQual'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'GarageCond'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m                                 'PoolQC', 'Fence', 'MiscFeature']\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconvert_nan_to_other_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: fillna() got an unexpected keyword argument 'subset'"
     ]
    }
   ],
   "source": [
    "# Predict\n",
    "test_dataset = pd.read_csv('data/test.csv')\n",
    "#test_dataset = NaN_check(test_dataset)\n",
    "test_dataset = NaN_processing(test_dataset)\n",
    "X_sub, y_sub, test_dataset = data_preprocessing(test_dataset, mode='predict')\n",
    "\n",
    "y_pred = rndm_regressor.predict(X_sub)\n",
    "\n",
    "output = pd.DataFrame({'Id': test_dataset['Id'], 'SalePrice': y_pred})\n",
    "output.to_csv('my_submission.csv', index=False)\n",
    "print(\"Your submission was successfully saved!\")"
   ]
  }
 ]
}