{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 81)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Raw dataset shape: 1460 * 81\n"
     ]
    }
   ],
   "source": [
    "# trainデータを取得、NaN値などの状況を確認\n",
    "dataset = pd.read_csv('data/train.csv', sep=',')\n",
    "test_dataset = pd.read_csv('data/test.csv', sep=',')\n",
    "dataset_header = dataset.columns\n",
    "print(f'Raw dataset shape: {dataset.shape[0]} * {dataset.shape[1]}')\n",
    "# NaNチェック\n",
    "# datasetから一列ずつ取り出して、nullが含まれている場合はそのカラム名を返す\n",
    "def print_NaN_information(dataset):\n",
    "    for data in dataset:\n",
    "        numof_null = dataset[data].isnull().sum()\n",
    "        if numof_null > 0:\n",
    "            ratio = (numof_null / dataset.shape[0]) * 100\n",
    "            print(f'NaN in {data}:{numof_null} \\tNaN, ratio is\\t{ratio:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nan_imputer(dataset):\n",
    "    # NaNデータの処理\n",
    "    # 以下のカラムリストについては、NAのものは\"None\"に置き換えることで欠損値を補完する\n",
    "    convert_nan_to_other_list = ['Alley', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2',\n",
    "                                'FireplaceQu', 'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond',\n",
    "                                'PoolQC', 'Fence', 'MiscFeature']\n",
    "    convert_nan_to_other_list = dict.fromkeys(convert_nan_to_other_list, 'None')\n",
    "    dataset = dataset.fillna(convert_nan_to_other_list)\n",
    "\n",
    "    # ******************************************************************************#\n",
    "    # Basementの処理。BsmtQual, BsmtCond, BsmtExposure, BsmtFinType1のいずれかが'None'なら、\n",
    "    # BsmtFinType2はNoneに、BsmtFinSF1とBsmtFinSF2, BsmtUnfSF, TotalBsmtSFは0となる\n",
    "    # どれかから地下室がない情報が得られていて、もし他のデータに齟齬や欠損があれば、その情報で補完\n",
    "\n",
    "    # BsmtFinType1がUnfなら、BsmtFinSF1は0となる\n",
    "    # BsmtFinType2がUnfなら、BsmtFinSF2は0となる\n",
    "    # ベースメントの建設がUnfなら、完成エリアの面積は0で補完\n",
    "    basement_condition = (dataset['BsmtQual'] == 'None')| (dataset['BsmtCond'] == 'None') | (dataset['BsmtExposure'] == 'None') | (dataset['BsmtFinType1'] == 'None')\n",
    "    basement_target = ['BsmtFinSF1', 'BsmtFinType2', 'BsmtUnfSF', 'TotalBsmtSF']\n",
    "    dataset.loc[basement_condition, basement_target] = 0\n",
    "    dataset.loc[basement_condition, ['BsmtFinType2']] = 'None'\n",
    "\n",
    "    dataset.loc[(dataset['BsmtFinType1'] == 'Unf'), ['BsmtFinSF1']] = 0\n",
    "    dataset.loc[(dataset['BsmtFinType2'] == 'Unf'), ['BsmtFinSF2']] = 0\n",
    "    # ******************************************************************************#\n",
    "\n",
    "    # LotFrontageのNAは、接続無しとして0で置換\n",
    "    dataset[['LotFrontage']] = dataset['LotFrontage'].fillna(0)\n",
    "\n",
    "    # ******************************************************************************#\n",
    "    # Garageの処理。GarageTypeがNAの場合は製造年は最古 - 100年とする\n",
    "    # nanとの比較を行うときに、 dataset['aaa'] == np.nan としても、正しくnanを検出できないので避けること\n",
    "    dataset.loc[(dataset['GarageType'].isnull() | (dataset['GarageType'] == 'None')), 'GarageYrBlt'] = dataset['GarageYrBlt'].dropna().value_counts().idxmin() - 100\n",
    "    # ******************************************************************************#\n",
    "\n",
    "    # ******************************************************************************#\n",
    "    # その他の欠損値処理\n",
    "    # いずれも名義尺度のため、NaNについては最頻値で補完してみる\n",
    "    dataset.loc[dataset['MasVnrType'].isnull(), 'MasVnrType'] = dataset['MasVnrType'].dropna().value_counts().idxmax()\n",
    "    dataset.loc[dataset['MasVnrArea'].isnull(), 'MasVnrArea'] = dataset['MasVnrArea'].dropna().value_counts().idxmax()\n",
    "    dataset.loc[dataset['Electrical'].isnull(), 'Electrical'] = dataset['Electrical'].dropna().value_counts().idxmax()\n",
    "    # ******************************************************************************#\n",
    "\n",
    "    print_NaN_information(dataset)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "dataset = nan_imputer(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Data Scalling\n",
    "# 数値尺度のカラム\n",
    "num_cols = ['LotFrontage', 'LotArea', 'YearBuilt', 'YearRemodAdd', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF',\n",
    "                '1stFlrSF', '2ndFlrSF', 'LowQualFinSF', 'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath', 'TotRmsAbvGrd', \n",
    "                'Fireplaces', 'GarageCars', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch','ScreenPorch',\n",
    "                'PoolArea', 'MiscVal']\n",
    "# 名義尺度のカラム。Idはいずれでもないのでpreprocessingの対象外。SalePriceはターゲットのため対象外。\n",
    "nominal_cols = [i for i in dataset.columns if i not in np.append(num_cols, ['Id', 'SalePrice'])]\n",
    "\n",
    "def data_preprocessing(dataset):\n",
    "\n",
    "    # ******************************************************************************#\n",
    "    # Yearの処理\n",
    "    this_year = 2021\n",
    "    dataset['GarageYrBlt'] = this_year - dataset['GarageYrBlt']\n",
    "    dataset['YrSold'] = this_year - dataset['YrSold']\n",
    "    # ******************************************************************************#\n",
    "\n",
    "    # 数値尺度たちは外れ値も考慮してZ-Scoreでスケーリング\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    dataset[num_cols] = scaler.fit_transform(dataset.loc[:,num_cols].values)\n",
    "    # 名義尺度たちはOne-Hotエンコーディング。他の値で残りの1つはわかるので、drop_first=True\n",
    "    dataset = pd.get_dummies(dataset, columns=nominal_cols, drop_first=True)\n",
    "    return dataset\n",
    "\n",
    "dataset = data_preprocessing(dataset)\n",
    "dataset.to_csv('preprocessed.csv')\n",
    "print_NaN_information(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Selection\n",
    "def get_train_test_data(dataset):\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X = dataset.drop(['Id', 'SalePrice'], axis=1).values\n",
    "    Id = dataset['Id']\n",
    "    if 'SalePrice' in dataset:\n",
    "        y = dataset['SalePrice'].values\n",
    "    else:\n",
    "        y = np.nan\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test, Id\n",
    "\n",
    "X_train, X_test, y_train, y_test, Id = get_train_test_data(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "R2 Score(train) is 0.95596\nR2 Score(test) is -1500861377121772800.00000\n"
     ]
    }
   ],
   "source": [
    "# (Multiple) Linear Regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(X_train,y_train)\n",
    "\n",
    "y_pred = regressor.predict(X_test)\n",
    "y_train_pred = regressor.predict(X_train)\n",
    "\n",
    "# trainデータとtestデータそれぞれのR2スコアを表示する関数\n",
    "def print_r2_score(y_train, y_test, y_train_pred, y_pred):\n",
    "    from sklearn.metrics import r2_score\n",
    "    print(f'R2 Score(train) is {r2_score(y_train, y_train_pred):.5f}')\n",
    "    print(f'R2 Score(test) is {r2_score(y_test, y_pred):.5f}')\n",
    "\n",
    "print_r2_score(y_train, y_test, y_train_pred, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "R2 Score(train) is 1.00000\nR2 Score(test) is 0.70588\n"
     ]
    }
   ],
   "source": [
    "# Polymial Regression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "polynomial_instance = PolynomialFeatures(degree = 2)\n",
    "X_poly_train = polynomial_instance.fit_transform(X_train)\n",
    "X_poly_test = polynomial_instance.fit_transform(X_test)\n",
    "\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(X_poly_train, y_train)\n",
    "y_train_pred = regressor.predict(X_poly_train)\n",
    "y_pred = regressor.predict(X_poly_test)\n",
    "\n",
    "print_r2_score(y_train, y_test, y_train_pred, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "R2 Score(train) is 0.08158\nR2 Score(test) is 0.07970\n"
     ]
    }
   ],
   "source": [
    "# SVR\n",
    "from sklearn.svm import SVR\n",
    "svr_regressor = SVR(kernel='linear')\n",
    "svr_regressor.fit(X_train, y_train)\n",
    "y_pred = svr_regressor.predict(X_test)\n",
    "y_train_pred = svr_regressor.predict(X_train)\n",
    "\n",
    "print_r2_score(y_train, y_test, y_train_pred, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "R2 Score(train) is -0.05313\nR2 Score(test) is -0.06164\n"
     ]
    }
   ],
   "source": [
    "# SVR (rbf)\n",
    "from sklearn.svm import SVR\n",
    "svr_regressor = SVR(kernel='rbf')\n",
    "svr_regressor.fit(X_train, y_train)\n",
    "y_pred = svr_regressor.predict(X_test)\n",
    "y_train_pred = svr_regressor.predict(X_train)\n",
    "\n",
    "print_r2_score(y_train, y_test, y_train_pred, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "R2 Score(train) is 0.97790\nR2 Score(test) is 0.85959\n"
     ]
    }
   ],
   "source": [
    "# Random Forest\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rndm_regressor = RandomForestRegressor(n_estimators = 100, random_state=0)\n",
    "rndm_regressor.fit(X_train, y_train)\n",
    "y_pred = rndm_regressor.predict(X_test)\n",
    "y_train_pred = rndm_regressor.predict(X_train)\n",
    "W\n",
    "print_r2_score(y_train, y_test, y_train_pred, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "Classification metrics can't handle a mix of multiclass and continuous targets",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"C:\\Users\\tochigi.yusuke\\.conda\\envs\\digit-reg\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 431, in _process_worker\n    r = call_item()\n  File \"C:\\Users\\tochigi.yusuke\\.conda\\envs\\digit-reg\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 285, in __call__\n    return self.fn(*self.args, **self.kwargs)\n  File \"C:\\Users\\tochigi.yusuke\\.conda\\envs\\digit-reg\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 595, in __call__\n    return self.func(*args, **kwargs)\n  File \"C:\\Users\\tochigi.yusuke\\.conda\\envs\\digit-reg\\lib\\site-packages\\joblib\\parallel.py\", line 263, in __call__\n    for func, args, kwargs in self.items]\n  File \"C:\\Users\\tochigi.yusuke\\.conda\\envs\\digit-reg\\lib\\site-packages\\joblib\\parallel.py\", line 263, in <listcomp>\n    for func, args, kwargs in self.items]\n  File \"C:\\Users\\tochigi.yusuke\\.conda\\envs\\digit-reg\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 560, in _fit_and_score\n    test_scores = _score(estimator, X_test, y_test, scorer)\n  File \"C:\\Users\\tochigi.yusuke\\.conda\\envs\\digit-reg\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 607, in _score\n    scores = scorer(estimator, X_test, y_test)\n  File \"C:\\Users\\tochigi.yusuke\\.conda\\envs\\digit-reg\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 88, in __call__\n    *args, **kwargs)\n  File \"C:\\Users\\tochigi.yusuke\\.conda\\envs\\digit-reg\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 213, in _score\n    **self._kwargs)\n  File \"C:\\Users\\tochigi.yusuke\\.conda\\envs\\digit-reg\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 72, in inner_f\n    return f(**kwargs)\n  File \"C:\\Users\\tochigi.yusuke\\.conda\\envs\\digit-reg\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 187, in accuracy_score\n    y_type, y_true, y_pred = _check_targets(y_true, y_pred)\n  File \"C:\\Users\\tochigi.yusuke\\.conda\\envs\\digit-reg\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 91, in _check_targets\n    \"and {1} targets\".format(type_true, type_pred))\nValueError: Classification metrics can't handle a mix of multiclass and continuous targets\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-381-f88077a80fcb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m ]\n\u001b[0;32m     12\u001b[0m \u001b[0mgrid_search\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mRandomForestRegressor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrid_parameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mgrid_search\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[0mgrid_search\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\digit-reg\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     70\u001b[0m                           FutureWarning)\n\u001b[0;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\digit-reg\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    734\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    735\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 736\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    737\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    738\u001b[0m         \u001b[1;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\digit-reg\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1186\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1187\u001b[0m         \u001b[1;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1188\u001b[1;33m         \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1189\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\digit-reg\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[1;34m(candidate_params)\u001b[0m\n\u001b[0;32m    713\u001b[0m                                \u001b[1;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    714\u001b[0m                                in product(candidate_params,\n\u001b[1;32m--> 715\u001b[1;33m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[0;32m    716\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    717\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\digit-reg\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1052\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1054\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1055\u001b[0m             \u001b[1;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1056\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\digit-reg\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    931\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    932\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'supports_timeout'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 933\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    934\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    935\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\digit-reg\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[1;34m(future, timeout)\u001b[0m\n\u001b[0;32m    540\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[0;32m    541\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 542\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    543\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mCfTimeoutError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\digit-reg\\lib\\concurrent\\futures\\_base.py\u001b[0m in \u001b[0;36mresult\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    423\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    424\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 425\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    426\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    427\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\digit-reg\\lib\\concurrent\\futures\\_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    382\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__get_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    383\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 384\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    385\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    386\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Classification metrics can't handle a mix of multiclass and continuous targets"
     ]
    }
   ],
   "source": [
    "# grid search\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "grid_parameters = [\n",
    "    {'n_estimators': [1, 2, 5, 10, 100, 1000], \n",
    "    'criterion': ['mse', 'mae'],\n",
    "    'min_samples_split': [1, 2, 5, 10, 20],\n",
    "    'min_samples_leaf': [1, 2, 5, 10, 20],\n",
    "    'bootstrap': [True, False],\n",
    "    'max_features': ['auto', 'sqrt', 'log2']\n",
    "    }\n",
    "]\n",
    "grid_search = GridSearchCV(RandomForestRegressor(), grid_parameters, cv=5, scoring='r2', n_jobs = -1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "fillna() got an unexpected keyword argument 'subset'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-281-83bdbe6f6db9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mtest_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data/test.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#test_dataset = NaN_check(test_dataset)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mtest_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNaN_processing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mX_sub\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_sub\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_preprocessing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'predict'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-c35e88a8bfbb>\u001b[0m in \u001b[0;36mNaN_processing\u001b[1;34m(dataset)\u001b[0m\n\u001b[0;32m      5\u001b[0m                                 \u001b[1;34m'FireplaceQu'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'GarageType'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'GarageFinish'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'GarageQual'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'GarageCond'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m                                 'PoolQC', 'Fence', 'MiscFeature']\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconvert_nan_to_other_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: fillna() got an unexpected keyword argument 'subset'"
     ]
    }
   ],
   "source": [
    "# Predict\n",
    "test_dataset = pd.read_csv('data/test.csv')\n",
    "#test_dataset = NaN_check(test_dataset)\n",
    "test_dataset = NaN_processing(test_dataset)\n",
    "X_sub, y_sub, test_dataset = data_preprocessing(test_dataset, mode='predict')\n",
    "\n",
    "y_pred = rndm_regressor.predict(X_sub)\n",
    "\n",
    "output = pd.DataFrame({'Id': test_dataset['Id'], 'SalePrice': y_pred})\n",
    "output.to_csv('my_submission.csv', index=False)\n",
    "print(\"Your submission was successfully saved!\")"
   ]
  }
 ]
}