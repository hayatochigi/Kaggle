{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n# category variable\ncategory = ['cat0', 'cat1', 'cat2', 'cat3', 'cat4', 'cat5', 'cat6', 'cat7', 'cat8', 'cat9']\n\n# continuous variable\ncontinuous = ['cont0', 'cont1', 'cont2', 'cont3', 'cont4', 'cont5', 'cont6', \n              'cont7', 'cont8', 'cont9', 'cont10', 'cont11', 'cont12', 'cont13']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"#features = ['cat1', 'cont0', 'cat2', 'cont11', 'cont13', 'cat8', 'cont8', 'cont1', 'cat9',\n#            'cont9', 'cont5', 'cat3', 'cat0', 'cat6', 'cont3', 'cat5', 'cont4', 'cont2', 'cont12']\n\ntrain_dataset = pd.read_csv('../input/tabular-playground-series-feb-2021/train.csv')\ntest_data = pd.read_csv('../input/tabular-playground-series-feb-2021/test.csv')\ndataset = pd.concat([train_dataset, test_data])\n\n# あきらかな外れ値は削除\n# [166042]はあきらかな外れ値\noutlier=[166042]\nfor x in outlier:\n    dataset = dataset.loc[dataset['id'] != x, :]\n\n# idとtargetは避難させておく\nid = dataset['id']\ntarget = dataset['target']\n# 避難させたので遠慮なく削除\ndataset = dataset.drop(columns=['id', 'target'])\n# 相関が高いものだけを使用\n# この手法は有効だったと思うが、Catboostでのスコアは一定以上伸びなかった。\n\n# 重要度を確認するためにはLabelEncodingが有効\nfrom sklearn.preprocessing import LabelEncoder, RobustScaler, StandardScaler\nencoder = LabelEncoder()\nscaler = RobustScaler()\n\nfor x in dataset.columns:\n    # notebookみると、みんなLabelEncoder使っているなぁ\n    if dataset[x].dtype == object:\n        #dataset[x] = encoder.fit_transform(dataset[x])\n        dataset = pd.get_dummies(dataset, columns=[x], drop_first=True)\n        \ndataset[continuous] = scaler.fit_transform(dataset.loc[:,continuous].values)\n\n# データセットにidとtargetを元に戻して\ndataset = pd.concat([id,dataset,target], axis=1)\n# targetのあるなしでtrainとtestを分割\ntrain = dataset.loc[dataset['target'].notnull(), :]\ntest  = dataset.loc[dataset['target'].isnull(), :]\n\nX = train.drop(columns=['id', 'target'])\ny = train['target']\nX_prediction = test.drop(columns=['id', 'target'])\nprediction_id = test.loc[:,'id']\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Lightgbm Optuna\n\n#!pip install optuna \nimport optuna.integration.lightgbm as lgb\n\ntrains = lgb.Dataset(X_train, y_train)\ntests = lgb.Dataset(X_test, y_test)\n\nparams = {'objective': 'mean_squared_error',\n         'metric': 'rmse'}\n\nmodel = lgb.train(params, trains, valid_sets=tests, early_stopping_rounds=10)\nbest_params = model.params\n\n\nimport lightgbm as lgb\n\nregressor = lgb.train(best_params, trains, valid_sets=tests)\ny_pred = regressor.predict(X_test)\n\nfrom sklearn.metrics import mean_squared_error\nprint(mean_squared_error(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import KFold\nimport lightgbm as lgb\n\nkf = KFold(n_splits=5,random_state=48,shuffle=True)\nrmse=[]  # list contains rmse for each fold\nn=0\npreds=0\n\n# kf.splitはindexを返すことに注意。データ自体じゃないよ!\nfor train, test in kf.split(X, y):\n    model = lgb.LGBMRegressor(**best_params)\n    x_tr = X.iloc[train, :].values\n    x_te = X.iloc[test,  :].values\n    y_tr = y.iloc[train].values\n    y_te = y.iloc[test].values\n    \n    model.fit(x_tr,y_tr,eval_set=[(x_te,y_te)],early_stopping_rounds=100,verbose=False)\n    rmse.append(mean_squared_error(y_te, model.predict(x_te), squared=False))\n    preds+=model.predict(X_prediction)\n    print(n+1,rmse[n])\n    n+=1\n\ny_pred = preds/kf.n_splits\nimport statistics as stat\nprint(f'RMSE mean = {stat.mean(rmse)}')\nprint(f'RMSE = {rmse}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## CatBoost Optuna\nfrom catboost import CatBoostRegressor\nfrom catboost import Pool\nfrom sklearn.metrics import r2_score, mean_squared_error\nimport optuna\n\ncat_train = Pool(X_train, y_train, cat_features=category)\ncat_test = Pool(X_test, y_test, cat_features=category)\n\ndef objective(trial):\n    params = {\n        'iterations': trial.suggest_int('iterations', 50, 500),\n        'depth': trial.suggest_int('depth', 4, 10),\n        'learning_rate': trial.suggest_loguniform('learning_rate', 0.001, 0.1),\n        'l2_leaf_reg': trial.suggest_int('l2_leaf', 1,10),\n        'eval_metric': trial.suggest_categorical('eval_metric', ['RMSE']),\n        'random_strength': trial.suggest_int('random_strength', 0, 100),\n        'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.01, 100.00),\n        'od_type': trial.suggest_categorical('od_type', ['IncToDec', 'Iter']),\n        'od_wait': trial.suggest_int('od_wait', 10, 50),\n        'use_best_model': True,\n        'cat_features': category\n    }  \n    \n    model = CatBoostRegressor(**params)\n    model.fit(cat_train, eval_set = cat_test)\n    y_pred = model.predict(cat_test)\n    \n    return np.sqrt(mean_squared_error(y_test, y_pred))\n\nstudy = optuna.create_study()\nstudy.optimize(objective, n_trials=20, n_jobs=-1)\ncat_best = study.best_params\nprint(cat_best)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.metrics import r2_score, mean_squared_error\n\nclass AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, models):\n        self.models = models\n    \n    def fit(self, X, y, X_test, y_test):\n        self.models_ = [clone(x) for x in self.models]\n\n        for model in self.models_:\n            # CatBoostに関しては、eval_setが必須のためここでエラーが発生する。\n            # その場合はexceptでfit処理を行う\n            print(f'************ RUNNING : {model} ************')\n            try:\n                model.fit(X, y)\n            except:\n                model.fit(X, y, eval_set=(X_test, y_test))\n                \n            y_pred = model.predict(X_test)\n            print(f'{model} RMSE: {mean_squared_error(y_test, y_pred)}')\n            \n        return self\n    \n    def predict(self, X):\n        predictions = np.column_stack(\n            [model.predict(X) for model in self.models_]\n            )\n        return np.mean(predictions, axis=1)\n\n    \naveraged_models = AveragingModels(models = (cat, lasso, lgb_tuned))\naveraged_models.fit(X_train, y_train, X_test, y_test)\n\n# 予想してみる\nfrom sklearn.metrics import mean_squared_error\ny_pred = averaged_models.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\n\ndef rmse(y_true, y_pred):\n    return tf.sqrt(tf.losses.mean_squared_error(y_true, y_pred))\n\n### ***********************************************************###\nmodel = tf.keras.Sequential()\n\n#model.add(tf.keras.layers.Dense(1024, activation='relu', input_shape=(X.shape[1],)))\n#model.add(tf.keras.layers.LeakyReLU())\n#model.add(tf.keras.layers.BatchNormalization())\n#model.add(tf.keras.layers.Dropout(0.5))\n\n#model.add(tf.keras.layers.Dense(512, activation='relu'))\nmodel.add(tf.keras.layers.Dense(64, kernel_regularizer=keras.regularizers.l2(0.001), activation='relu',input_shape=(X.shape[1],)))\nmodel.add(tf.keras.layers.LeakyReLU())\nmodel.add(tf.keras.layers.BatchNormalization())\nmodel.add(tf.keras.layers.Dropout(0.4))\n\n#model.add(tf.keras.layers.Dense(512, activation='relu'))\nmodel.add(tf.keras.layers.Dense(64, kernel_regularizer=keras.regularizers.l2(0.001), activation='relu'))\nmodel.add(tf.keras.layers.LeakyReLU())\nmodel.add(tf.keras.layers.BatchNormalization())\nmodel.add(tf.keras.layers.Dropout(0.4))\n\n#model.add(tf.keras.layers.Dense(units=256, activation='relu'))\nmodel.add(tf.keras.layers.Dense(32, kernel_regularizer=keras.regularizers.l2(0.001), activation='relu'))\nmodel.add(tf.keras.layers.LeakyReLU())\nmodel.add(tf.keras.layers.BatchNormalization())\nmodel.add(tf.keras.layers.Dropout(0.3))\n\n#model.add(tf.keras.layers.Dense(units=256, activation='relu'))\nmodel.add(tf.keras.layers.Dense(32, kernel_regularizer=keras.regularizers.l2(0.001), activation='relu'))\nmodel.add(tf.keras.layers.LeakyReLU())\nmodel.add(tf.keras.layers.BatchNormalization())\nmodel.add(tf.keras.layers.Dropout(0.3))\n\n#model.add(tf.keras.layers.Dense(units=128, activation='relu'))\nmodel.add(tf.keras.layers.Dense(16, kernel_regularizer=keras.regularizers.l2(0.001), activation='relu'))\nmodel.add(tf.keras.layers.LeakyReLU())\n\nmodel.add(tf.keras.layers.Dense(units=1, activation='linear'))\n### ***********************************************************###\n\noptimizer = tf.keras.optimizers.Adam(lr=0.005, decay=5e-4)\nmodel.compile(optimizer = optimizer, loss = 'mae', metrics = ['mse', 'mae'])\n\n#checkpoint_name = 'Model/{epoch:03d}-{val_loss:.5f}.hdf5'\ncheckpoint_name = 'DNN_BestModel.hdf5'\ncheckpoint = tf.keras.callbacks.ModelCheckpoint(checkpoint_name, monitor='val_loss', verbose=1, save_best_only=True, mode='auto')\ncallback_list = [checkpoint]\n\nhistory = model.fit(X_train, y_train, validation_split=0.2, epochs = 500, batch_size = 1024,\n                    validation_data=(X_test, y_test), callbacks=callback_list)\ny_pred = model.predict(X_test).reshape(-1)\nRMSE = rmse(y_test, y_pred)\nprint(f'RMSE = {RMSE}')\n\nfig = plt.figure(figsize=(8,5))\nax = fig.add_subplot(111)\nax.set_ylim(0.6, 0.75)\n\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.legend(['Train', 'Test'], loc='upper right')\nplt.savefig(fname='1024 neurons enable LearningRate.png')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"# CatBoosting\n'''\nfrom catboost import CatBoostRegressor\nfrom sklearn.metrics import r2_score, mean_squared_error\n\nfrom sklearn.model_selection import StratifiedKFold, KFold\nkfold = KFold(n_splits=5, shuffle=True, random_state=42)\n\nfrom sklearn.model_selection import GridSearchCV\nparam_cat = {'depth':[6,8,10],\n            'learning_rate':[0.005, 0.001],\n            'l2_leaf_reg':[1,4,9],\n            'iterations':[100],\n            'cat_features':[feature_cat],\n            'eval_metric':['RMSE']\n            }\n\ngrid_result = GridSearchCV(estimator=CatBoostRegressor(),param_grid=param_cat, cv=kfold, scoring='neg_mean_squared_error', n_jobs = -1, verbose=2)\ngrid_result.fit(X_train, y_train)\ngrid_param = grid_result.best_params_\nprint(grid_param)\n\n\ncat = CatBoostRegressor(task_type='GPU', iterations=8000, use_best_model=True, depth=10, eval_metric='RMSE', l2_leaf_reg=1, learning_rate=0.001, early_stopping_rounds=10)\ncat.fit(X_train, y_train, cat_features=feature_cat, eval_set = (X_test, y_test))\ny_pred = cat.predict(X_test)\nprint(f'RMSE: {mean_squared_error(y_test, y_pred)}')\n\n\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#y_pred=regressor.predict(X_prediction)\noutput = pd.DataFrame({'id': prediction_id, 'target': y_pred})\noutput.to_csv('my_submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}