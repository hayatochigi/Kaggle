{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom imblearn.over_sampling import SMOTE, SMOTENC","execution_count":1,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Very very basic data ingest\n\nds_tr = pd.read_csv('../input/tabular-playground-series-mar-2021/train.csv')\n\nfig = plt.subplots(figsize=(8,6))\nds_tr['target'].hist()\n\nif ds_tr.isnull().sum().sum() != 0:\n    print('******\\n There might be NULL error! \\n******')\n\nid = ds_tr['id']\ntarget = ds_tr['target']\nds_tr = ds_tr.drop(columns=['id', 'target'], axis=1)\nds_tr.head()\ncat_col = []\nnum_col = []\nfor col in ds_tr.columns:\n    if np.dtype(ds_tr[col]) == 'object':\n        cat_col.append(col)\n    else:\n        num_col.append(col)\n\nprint(f'Categorical is {cat_col}')\nprint(f'Numerical is {num_col}')","execution_count":2,"outputs":[{"output_type":"stream","text":"Categorical is ['cat0', 'cat1', 'cat2', 'cat3', 'cat4', 'cat5', 'cat6', 'cat7', 'cat8', 'cat9', 'cat10', 'cat11', 'cat12', 'cat13', 'cat14', 'cat15', 'cat16', 'cat17', 'cat18']\nNumerical is ['cont0', 'cont1', 'cont2', 'cont3', 'cont4', 'cont5', 'cont6', 'cont7', 'cont8', 'cont9', 'cont10']\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"<Figure size 576x432 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Visualize dataset\n# In categorical, use histogram\n'''\nfig, axes = plt.subplots(5,4, figsize=(20,10))\nfor i, cat in enumerate(cat_col):\n    row = int(i/4)\n    col = int(i%4)\n    axes[row, col].set_title(cat)\n    plt.tight_layout()\n    ds_tr[cat].hist(ax=axes[row, col])\n\n# In numerical, use scatter\nfig, axes = plt.subplots(3,4, figsize=(20,10))\nfor i, num in enumerate(num_col):\n    row = int(i/4)\n    col = int(i%4)\n    axes[row, col].set_title(num)\n    plt.tight_layout()\n    ds_tr[num].plot(ax=axes[row, col])\n'''","execution_count":3,"outputs":[{"output_type":"execute_result","execution_count":3,"data":{"text/plain":"'\\nfig, axes = plt.subplots(5,4, figsize=(20,10))\\nfor i, cat in enumerate(cat_col):\\n    row = int(i/4)\\n    col = int(i%4)\\n    axes[row, col].set_title(cat)\\n    plt.tight_layout()\\n    ds_tr[cat].hist(ax=axes[row, col])\\n\\n# In numerical, use scatter\\nfig, axes = plt.subplots(3,4, figsize=(20,10))\\nfor i, num in enumerate(num_col):\\n    row = int(i/4)\\n    col = int(i%4)\\n    axes[row, col].set_title(num)\\n    plt.tight_layout()\\n    ds_tr[num].plot(ax=axes[row, col])\\n'"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"### Too many bins at ['cat5', 'cat7', 'cat8', 'cat10']\n>cat5 has 84 categorical value.  \ncat7 has 51 categorical value.  \ncat8 has 61 categorical value.  \ncat10 has 299 categorical value.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"toomany_col = ['cat5', 'cat7', 'cat8', 'cat10']\nfor x in toomany_col:\n    print(f'{x} has {ds_tr.loc[:,x].value_counts().count()} categorical value.')","execution_count":4,"outputs":[{"output_type":"stream","text":"cat5 has 84 categorical value.\ncat7 has 51 categorical value.\ncat8 has 61 categorical value.\ncat10 has 299 categorical value.\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# SMOTE \nTo resolve imbalanced data, try [SMOTENC](https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.SMOTENC.html)."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\nfeatures = cat_col + num_col\n\nX = ds_tr.loc[:,features]\ny = target\nprint(f'Orignal X size = {X.size}')\n\ncat_indices = np.arange(0, len(cat_col),1)\nsm = SMOTENC(categorical_features=cat_indices, k_neighbors=10, n_jobs=-1, random_state=42)\nX, y = sm.fit_resample(X,y)\nprint(f'SMOTED X size = {X.size}')\n\n## If CatBoost, don't feature engineering at here!! Specify categorical columns in CatBoost!!\nX = pd.get_dummies(X,columns=cat_col, drop_first=True)","execution_count":5,"outputs":[{"output_type":"stream","text":"Orignal X size = 9000000\nSMOTED X size = 13232340\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"\n"},{"metadata":{},"cell_type":"markdown","source":"# Lightgbm Classification\n\n* 2021/3/3\n> {'lambda_l1': 0.05257926843321973, 'lambda_l2': 3.3665051868901688e-06, 'num_leaves': 256, 'feature_fraction': 0.5340551354973648, 'bagging_fraction': 0.9375929208450426, 'bagging_freq': 3, 'min_child_samples': 21, 'learning_rage': 0.009708807177251682}\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"## LightGBM Classification\nbest_param = {'lambda_l1': 0.05257926843321973, 'lambda_l2': 3.3665051868901688e-06, 'num_leaves': 256, 'feature_fraction': 0.5340551354973648, 'bagging_fraction': 0.9375929208450426, 'bagging_freq': 3, 'min_child_samples': 21, 'learning_rage': 0.009708807177251682}\n\nimport optuna\nimport lightgbm as lgb\nfrom sklearn.metrics import roc_auc_score\n\nfrom sklearn.model_selection import train_test_split\nX_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.3)\n\n'''\ndef objective(trial):\n    params = {\n            'objective': 'binary',\n            'metric': 'auc',\n            'n_estimators': 1000,\n            'lambda_l1': trial.suggest_loguniform('lambda_l1', 1e-8, 10.0),\n            'lambda_l2': trial.suggest_loguniform('lambda_l2', 1e-8, 10.0),\n            'num_leaves': trial.suggest_int('num_leaves', 2, 256),\n            'feature_fraction': trial.suggest_uniform('feature_fraction', 0.4, 1.0),\n            'bagging_fraction': trial.suggest_uniform('bagging_fraction', 0.4, 1.0),\n            'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),\n            'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n            'learning_rate': trial.suggest_loguniform('learning_rage', 1e-6, 1e-2)\n        }\n    \n    lgb_classifier = lgb.LGBMClassifier(**params)\n    lgb_classifier.fit(X=X_tr, y=y_tr, eval_set=[(X_te, y_te)],eval_metric='auc', \n                       early_stopping_rounds=10,verbose=False)\n    y_pred = lgb_classifier.predict_proba(X_te, num_iteration=lgb_classifier.best_iteration_)[:,1]\n    \n    return 1 - roc_auc_score(y_te, y_pred)\n    \nlgb_train = lgb.Dataset(X_tr, y_tr)\nlgb_test = lgb.Dataset(X_te, y_te)\n\nstudy = optuna.create_study()\nstudy.optimize(objective, n_trials=100, n_jobs=-1)\nlgb_best = study.best_params\nprint(lgb_best)\n'''","execution_count":12,"outputs":[{"output_type":"execute_result","execution_count":12,"data":{"text/plain":"\"\\ndef objective(trial):\\n    params = {\\n            'objective': 'binary',\\n            'metric': 'auc',\\n            'n_estimators': 1000,\\n            'lambda_l1': trial.suggest_loguniform('lambda_l1', 1e-8, 10.0),\\n            'lambda_l2': trial.suggest_loguniform('lambda_l2', 1e-8, 10.0),\\n            'num_leaves': trial.suggest_int('num_leaves', 2, 256),\\n            'feature_fraction': trial.suggest_uniform('feature_fraction', 0.4, 1.0),\\n            'bagging_fraction': trial.suggest_uniform('bagging_fraction', 0.4, 1.0),\\n            'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),\\n            'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\\n            'learning_rate': trial.suggest_loguniform('learning_rage', 1e-6, 1e-2)\\n        }\\n    \\n    lgb_classifier = lgb.LGBMClassifier(**params)\\n    lgb_classifier.fit(X=X_tr, y=y_tr, eval_set=[(X_te, y_te)],eval_metric='auc', \\n                       early_stopping_rounds=10,verbose=False)\\n    y_pred = lgb_classifier.predict_proba(X_te, num_iteration=lgb_classifier.best_iteration_)[:,1]\\n    \\n    return 1 - roc_auc_score(y_te, y_pred)\\n    \\nlgb_train = lgb.Dataset(X_tr, y_tr)\\nlgb_test = lgb.Dataset(X_te, y_te)\\n\\nstudy = optuna.create_study()\\nstudy.optimize(objective, n_trials=100, n_jobs=-1)\\nlgb_best = study.best_params\\nprint(lgb_best)\\n\""},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"# CatBoost Classification\n* category columnsをfit時に選択すると、時間むっちゃかかる\n* SMOTENC + CatBoostでは、 \"Best is trial 45 with value: 0.13424544780540504.\" の程度"},{"metadata":{"trusted":true},"cell_type":"code","source":"## CatBoost Classification\n'''\nimport optuna\nfrom catboost import CatBoostClassifier\nfrom sklearn.metrics import roc_auc_score\n\ndef objective(trial):\n    params = {\n        #'iterations' : trial.suggest_int('iterations', 50, 3000),\n        'iterations' : 1000,\n        'loss_function': 'CrossEntropy',\n        'depth' : trial.suggest_int('depth', 4, 10),                                       \n        'learning_rate' : trial.suggest_loguniform('learning_rate', 1e-6, 0.1),               \n        'random_strength' :trial.suggest_int('random_strength', 0, 100),                       \n        'bagging_temperature' :trial.suggest_loguniform('bagging_temperature', 0.01, 100.00), \n        'od_type': trial.suggest_categorical('od_type', ['IncToDec', 'Iter']),\n        'od_wait' :trial.suggest_int('od_wait', 10, 50)\n    }\n\n    # 学習\n    model = CatBoostClassifier(**params)\n    \n    model.fit(X=X_tr, y=y_tr, eval_set=[(X_te, y_te)], use_best_model=True, early_stopping_rounds=50, verbose=False)\n    y_pred = model.predict(X_te)\n    \n    return 1 - roc_auc_score(y_te, y_pred)\n    \nfrom sklearn.model_selection import train_test_split\nX_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.3)\n\nstudy = optuna.create_study()\nstudy.optimize(objective, n_trials=100, n_jobs=-1)\ncat_best = study.best_params\nprint(cat_best)\n'''","execution_count":7,"outputs":[{"output_type":"execute_result","execution_count":7,"data":{"text/plain":"\"\\nimport optuna\\nfrom catboost import CatBoostClassifier\\nfrom sklearn.metrics import roc_auc_score\\n\\ndef objective(trial):\\n    params = {\\n        #'iterations' : trial.suggest_int('iterations', 50, 3000),\\n        'iterations' : 1000,\\n        'loss_function': 'CrossEntropy',\\n        'depth' : trial.suggest_int('depth', 4, 10),                                       \\n        'learning_rate' : trial.suggest_loguniform('learning_rate', 1e-6, 0.1),               \\n        'random_strength' :trial.suggest_int('random_strength', 0, 100),                       \\n        'bagging_temperature' :trial.suggest_loguniform('bagging_temperature', 0.01, 100.00), \\n        'od_type': trial.suggest_categorical('od_type', ['IncToDec', 'Iter']),\\n        'od_wait' :trial.suggest_int('od_wait', 10, 50)\\n    }\\n\\n    # 学習\\n    model = CatBoostClassifier(**params)\\n    \\n    model.fit(X=X_tr, y=y_tr, eval_set=[(X_te, y_te)], use_best_model=True, early_stopping_rounds=50, verbose=False)\\n    y_pred = model.predict(X_te)\\n    \\n    return 1 - roc_auc_score(y_te, y_pred)\\n    \\nfrom sklearn.model_selection import train_test_split\\nX_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.3)\\n\\nstudy = optuna.create_study()\\nstudy.optimize(objective, n_trials=100, n_jobs=-1)\\ncat_best = study.best_params\\nprint(cat_best)\\n\""},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Predict\npredict_ds = pd.read_csv('../input/tabular-playground-series-mar-2021/test.csv')\nid = predict_ds['id']\npredict_ds = predict_ds.drop(columns=['id'], axis=1)\npredict_ds = pd.get_dummies(predict_ds, columns=cat_col, drop_first=True)\n\nlgb_classifier = lgb.LGBMClassifier(**best_param)\nlgb_classifier.fit(X=X_tr, y=y_tr, eval_set=[(X_te, y_te)],eval_metric='auc', \n                   early_stopping_rounds=10,verbose=False)\ny_pred = lgb_classifier.predict_proba(predict_ds, num_iteration=lgb_classifier.best_iteration_)[:,1]\n\n'''\npredictor = CatBoostClassifier(**cat_best)\npredictor.fit(X=X_tr, y=y_tr, eval_set=[(X_te, y_te)],cat_features=cat_col, \n               use_best_model=True, early_stopping_rounds=50, verbose=False)\n    \ny_pred = predictor(predict_ds)\n'''\n\noutput = pd.DataFrame({'id': id, 'target': y_pred})\noutput.to_csv('my_submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","execution_count":13,"outputs":[{"output_type":"stream","text":"[LightGBM] [Warning] Unknown parameter: learning_rage\n[LightGBM] [Warning] lambda_l1 is set=0.05257926843321973, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.05257926843321973\n[LightGBM] [Warning] bagging_fraction is set=0.9375929208450426, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9375929208450426\n[LightGBM] [Warning] lambda_l2 is set=3.3665051868901688e-06, reg_lambda=0.0 will be ignored. Current value: lambda_l2=3.3665051868901688e-06\n[LightGBM] [Warning] feature_fraction is set=0.5340551354973648, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5340551354973648\n[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n","name":"stdout"},{"output_type":"error","ename":"ValueError","evalue":"Number of features of the model must match the input. Model n_features_ is 615 and input n_features is 611 ","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-13-2c439383eea1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m lgb_classifier.fit(X=X_tr, y=y_tr, eval_set=[(X_te, y_te)],eval_metric='auc', \n\u001b[1;32m      9\u001b[0m                    early_stopping_rounds=10,verbose=False)\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlgb_classifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredict_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_iteration\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlgb_classifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_iteration_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m '''\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/lightgbm/sklearn.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[0;34m(self, X, raw_score, start_iteration, num_iteration, pred_leaf, pred_contrib, **kwargs)\u001b[0m\n\u001b[1;32m    921\u001b[0m         \"\"\"\n\u001b[1;32m    922\u001b[0m         result = super(LGBMClassifier, self).predict(X, raw_score, start_iteration, num_iteration,\n\u001b[0;32m--> 923\u001b[0;31m                                                      pred_leaf, pred_contrib, **kwargs)\n\u001b[0m\u001b[1;32m    924\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_objective\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mraw_score\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mpred_leaf\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mpred_contrib\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    925\u001b[0m             warnings.warn(\"Cannot compute class probabilities or labels \"\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/lightgbm/sklearn.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X, raw_score, start_iteration, num_iteration, pred_leaf, pred_contrib, **kwargs)\u001b[0m\n\u001b[1;32m    684\u001b[0m                              \u001b[0;34m\"match the input. Model n_features_ is %s and \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    685\u001b[0m                              \u001b[0;34m\"input n_features is %s \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m                              % (self._n_features, n_features))\n\u001b[0m\u001b[1;32m    687\u001b[0m         return self._Booster.predict(X, raw_score=raw_score, start_iteration=start_iteration, num_iteration=num_iteration,\n\u001b[1;32m    688\u001b[0m                                      pred_leaf=pred_leaf, pred_contrib=pred_contrib, **kwargs)\n","\u001b[0;31mValueError\u001b[0m: Number of features of the model must match the input. Model n_features_ is 615 and input n_features is 611 "]}]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}