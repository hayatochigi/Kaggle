{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport os\nimport tensorflow as tf","execution_count":1,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pathlib\ndata_folder = pathlib.Path('../input/flowers-recognition/flowers')\nflower_folders = []\nLabels = []\nfor item in data_folder.iterdir():\n    flower_folders.append(item)\n    # フォルダ名からラベルを取得\n    Labels.append(str(item).split('/')[-1])\n\nlabel_dictionary = dict(zip(Labels, np.arange(0,len(Labels))))\nprint(label_dictionary)\n\n# .jpgのファイル数をカウントして、ndarrayを初期化したい\ntotal_images_number = 0\nfor folder_path in flower_folders:\n    for file_path in folder_path.iterdir():\n        if os.path.splitext(file_path)[-1] == '.jpg':\n            total_images_number += 1\nprint(total_images_number)","execution_count":2,"outputs":[{"output_type":"stream","text":"{'dandelion': 0, 'daisy': 1, 'flowers': 2, 'sunflower': 3, 'tulip': 4, 'rose': 5}\n4323\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"image_size=256\nX = np.empty([total_images_number,image_size, image_size, 3])\ny = np.empty([0])\ncount = 0\nfrom PIL import Image\nfor folder_path in flower_folders:\n    for file_path in folder_path.iterdir():\n        if os.path.splitext(file_path)[-1] == '.jpg':\n            image = Image.open(file_path)\n            # 画像データをリサイズし、ndarrayへ変換\n            image_array = np.asarray(image.resize((image_size, image_size), Image.NEAREST))\n            image.close()\n            X[count,:,:,:] = image_array\n            # 末尾に(label列に)ディクショナリからの値を追加\n            folder_name = os.path.basename(os.path.dirname(file_path))\n            y = np.append(y,label_dictionary[folder_name])\n            count += 1\n    print(str(folder_name) + ' Completed:')","execution_count":3,"outputs":[{"output_type":"stream","text":"dandelion Completed:\ndaisy Completed:\ndaisy Completed:\nsunflower Completed:\ntulip Completed:\nrose Completed:\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ndarrayをtensorへ変換して、テストデータとする\nX_tensor = tf.convert_to_tensor(X)","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# CNNの初期化。単純なSequentialモデルで作成。\ncnn = tf.keras.models.Sequential()\n\n# 畳み込み https://keras.io/ja/layers/convolutional/\n# 128x128 RGB画像ではinput_shape=(128, 128, 3)となります．\n# 最初のレイヤーだけは、入り口となる入力シェイプが必要\ncnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, padding=\"same\", activation=\"relu\", input_shape=[image_size, image_size, 3], data_format='channels_last'))\n\n# Poolingにより、ダウンサンプリング\ncnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2, padding='valid'))\n\n# 2層目の中間層を追加\ncnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, padding=\"same\", activation=\"relu\"))\ncnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2, padding='valid'))\n\n# Flattening\ncnn.add(tf.keras.layers.Flatten())\n\n# 接続\n# unitsは   レイヤーの出力形状\ncnn.add(tf.keras.layers.Dense(units=128, activation='relu'))\n\n# 出力層\n#cnn.add(tf.keras.layers.Dense(units=10, activation='sigmoid'))\ncnn.add(tf.keras.layers.Dense(6, activation='softmax'))\n\n# Compiling the CNN\n# 評価関数の選定 -> https://keras.io/ja/metrics/\ncnn.compile(optimizer = 'adam', loss = tf.keras.losses.sparse_categorical_crossentropy, metrics = ['accuracy'])\n\nanswer = cnn.fit(X_tensor, y, epochs = 10, batch_size = 20)","execution_count":6,"outputs":[{"output_type":"stream","text":"Epoch 1/10\n217/217 [==============================] - 200s 924ms/step - loss: 73.0330 - accuracy: 0.2750\nEpoch 2/10\n217/217 [==============================] - 200s 923ms/step - loss: 1.3147 - accuracy: 0.4874\nEpoch 3/10\n217/217 [==============================] - 205s 944ms/step - loss: 0.8583 - accuracy: 0.6768\nEpoch 4/10\n217/217 [==============================] - 202s 930ms/step - loss: 0.5333 - accuracy: 0.8154\nEpoch 5/10\n217/217 [==============================] - 201s 927ms/step - loss: 0.5039 - accuracy: 0.8256\nEpoch 6/10\n217/217 [==============================] - 200s 923ms/step - loss: 0.4559 - accuracy: 0.8612\nEpoch 7/10\n217/217 [==============================] - 200s 920ms/step - loss: 0.3780 - accuracy: 0.8862\nEpoch 8/10\n217/217 [==============================] - 199s 915ms/step - loss: 0.3510 - accuracy: 0.8920\nEpoch 9/10\n217/217 [==============================] - 200s 922ms/step - loss: 0.2834 - accuracy: 0.9144\nEpoch 10/10\n217/217 [==============================] - 204s 938ms/step - loss: 0.2078 - accuracy: 0.9352\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"cnn.evaluate(X_tensor, y)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}