{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 81)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# trainデータを取得、NaN値などの状況を確認\n",
    "dataset = pd.read_csv('data/train.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Dataset doesnt have NaN\n"
     ]
    }
   ],
   "source": [
    "## NaNデータの処理\n",
    "# 以下のカラムリストについては、NAのものは\"None\"に置き換えることで欠損値を補完する\n",
    "dataset = pd.read_csv('data/train.csv', sep=',')\n",
    "\n",
    "def feature_engineering(dataset):\n",
    "    \n",
    "    # 明らかな外れ値についてはデータから削除\n",
    "    outliers = [ 524, 1299]\n",
    "    for x in outliers:\n",
    "        dataset = dataset.drop(dataset[dataset['Id'] == x].index)\n",
    "    \n",
    "    Id = dataset['Id']\n",
    "    dataset = dataset.drop(['Id'], axis=1)\n",
    "    if 'SalePrice' in dataset.columns:\n",
    "        y = dataset['SalePrice'].values\n",
    "        dataset = dataset.drop(['SalePrice'], axis=1)\n",
    "    else:\n",
    "        y = np.NaN\n",
    "\n",
    "    # ************************************************************* NaN処理 *************************************************************\n",
    "    convert_nan_to_other_list = ['MSSubClass', 'Alley', 'MasVnrType', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2',\n",
    "                                'FireplaceQu', 'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond',\n",
    "                                'PoolQC', 'Fence', 'MiscFeature']\n",
    "    convert_nan_to_other_list = dict.fromkeys(convert_nan_to_other_list, 'None')\n",
    "    dataset = dataset.fillna(convert_nan_to_other_list)\n",
    "\n",
    "    # LotFrontageのNAは、各近接地における中央値で置換\n",
    "    dataset['LotFrontage'] = dataset['LotFrontage'].fillna(dataset.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median())))\n",
    "    # Functionalについてはdescriptionに\"Assume typical unless deductions are warranted\"とある\n",
    "    dataset['Functional'] = dataset['Functional'].fillna('Typ')\n",
    "\n",
    "    # ************************* 0でfillna *************************\n",
    "    for x in ('GarageYrBlt', 'GarageArea', 'GarageCars', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath', 'MasVnrArea'):\n",
    "        dataset[x] = dataset[x].fillna(0)\n",
    "\n",
    "    # ************************* 最頻値でfillna *************************\n",
    "    for x in ('MSZoning', 'Electrical', 'KitchenQual', 'Exterior1st', 'Exterior2nd', 'SaleType'):\n",
    "        dataset[x] = dataset[x].fillna(dataset[x].mode()[0])\n",
    "\n",
    "    dataset = dataset.drop(['Utilities'],axis=1)\n",
    "\n",
    "    if dataset.isnull().sum().sum() == 0:\n",
    "        print('Dataset doesnt have NaN')\n",
    "    # ************************************************************* NaN完了 *************************************************************\n",
    "\n",
    "\n",
    "    # *********************************************************** Feature Engineering *************************************************************\n",
    "    categorical = ['MSSubClass', 'OverallQual','OverallCond', 'YrSold', 'MoSold']\n",
    "    for x in categorical:\n",
    "        dataset[x] = dataset[x].astype(str)\n",
    "\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    label_encoder = LabelEncoder()\n",
    "    # データセットが情報を表す順序で並んでいる\"だろう\"ものについて、label encoding\n",
    "    label_encoding = ['MSSubClass', 'Street', 'Alley', 'LotShape', 'LandSlope', 'OverallQual', 'OverallCond', 'ExterQual', 'ExterCond', \n",
    "                    'BsmtQual', 'BsmtCond', 'BsmtExposure','BsmtFinType1', 'BsmtFinType2', 'HeatingQC', 'CentralAir', 'KitchenQual',\n",
    "                    'FireplaceQu', 'GarageFinish', 'GarageQual', 'GarageCond', 'PoolQC', 'MoSold', 'YrSold']\n",
    "    for x in label_encoding:\n",
    "        dataset[x] = label_encoder.fit_transform(dataset[x].values)\n",
    "\n",
    "    dataset['TotalSF'] = dataset['TotalBsmtSF'] + dataset['1stFlrSF'] + dataset['2ndFlrSF']\n",
    "    \n",
    "    # 数値尺度のカラムを取得\n",
    "    numeric_cols = dataset.dtypes[dataset.dtypes != 'object'].index\n",
    "    skew = dataset.loc[:,numeric_cols].apply(lambda x: x.skew()).sort_values(ascending=False)\n",
    "    # skewが中央にないものは、log変換\n",
    "    skewness = skew[abs(skew) > 0.75].index\n",
    "    dataset[skewness] = np.log1p(dataset[skewness])\n",
    "\n",
    "    onehot_encoding = dataset.dtypes[dataset.dtypes == 'object'].index\n",
    "    dataset = pd.get_dummies(dataset, columns=onehot_encoding, drop_first=True)\n",
    "    # ********************************************************* End of Feature Engineering **********************************************************\n",
    "\n",
    "    return Id, dataset.values, y\n",
    "\n",
    "Id, X, y = feature_engineering(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "score = : 0.85841015\n",
      "score = : 0.86091266\n",
      "score = : 0.82656832\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import RobustScaler, StandardScaler\n",
    "from sklearn.linear_model import Lasso, ElasticNet, BayesianRidge, LassoLarsIC\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "stratified = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "def cross_val(model):\n",
    "    score = cross_val_score(model, X, y, scoring='r2', cv=stratified)\n",
    "    print(f'score = : {score.mean():.8f}')\n",
    "\n",
    "lasso = make_pipeline(RobustScaler(), Lasso(alpha=0.0005, random_state=0))\n",
    "elastic = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=0)) \n",
    "kernel = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)\n",
    "\n",
    "for x in (lasso, elastic, kernel):\n",
    "    cross_val(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "R2 Score(train) is 0.91997\nR2 Score(test) is 0.84336\n"
     ]
    }
   ],
   "source": [
    "# (Multiple) Linear Regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(X_train,y_train)\n",
    "\n",
    "y_pred = regressor.predict(X_test)\n",
    "y_train_pred = regressor.predict(X_train)\n",
    "\n",
    "# trainデータとtestデータそれぞれのR2スコアを表示する関数\n",
    "def print_r2_score(y_train, y_test, y_train_pred, y_pred):\n",
    "    from sklearn.metrics import r2_score\n",
    "    print(f'R2 Score(train) is {r2_score(y_train, y_train_pred):.5f}')\n",
    "    print(f'R2 Score(test) is {r2_score(y_test, y_pred):.5f}')\n",
    "\n",
    "print_r2_score(y_train, y_test, y_train_pred, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "R2 Score(train) is 0.98457\nR2 Score(test) is 0.88754\n"
     ]
    }
   ],
   "source": [
    "# Random Forest\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rndm_regressor = RandomForestRegressor(n_estimators = 100, criterion='mae', random_state=0, bootstrap=True)\n",
    "rndm_regressor.fit(X_train, y_train)\n",
    "y_pred = rndm_regressor.predict(X_test)\n",
    "y_train_pred = rndm_regressor.predict(X_train)\n",
    "\n",
    "print_r2_score(y_train, y_test, y_train_pred, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.0620s.) Setting batch_size=2.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    2.5s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too slow (2.0833s.) Setting batch_size=1.\n",
      "[Parallel(n_jobs=-1)]: Done  52 tasks      | elapsed:    3.3s\n",
      "[Parallel(n_jobs=-1)]: Done  71 tasks      | elapsed:    5.9s\n",
      "[Parallel(n_jobs=-1)]: Done  85 out of 100 | elapsed:   10.4s remaining:    1.8s\n",
      "[Parallel(n_jobs=-1)]: Done  96 out of 100 | elapsed:   32.6s remaining:    1.3s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:   34.7s finished\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'bootstrap': True, 'criterion': 'mae', 'n_estimators': 100}"
      ]
     },
     "metadata": {},
     "execution_count": 48
    }
   ],
   "source": [
    "# grid search\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "grid_parameters = [\n",
    "    {'n_estimators': [1, 2, 5, 10, 100], \n",
    "    'criterion': ['mse', 'mae'],\n",
    "    #'min_samples_split': [1, 2, 5, 10, 20],\n",
    "    #'min_samples_leaf': [1, 2, 5, 10, 20],\n",
    "    'bootstrap': [True, False],\n",
    "    #'max_features': ['auto', 'sqrt', 'log2']\n",
    "    }\n",
    "]\n",
    "grid_search = GridSearchCV(RandomForestRegressor(), grid_parameters, cv=5, scoring='r2', n_jobs = -1, verbose=10)\n",
    "grid_search.fit(X_train, y_train)\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'NaN_processing' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-83bdbe6f6db9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mtest_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data/test.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#test_dataset = NaN_check(test_dataset)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mtest_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNaN_processing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mX_sub\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_sub\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_preprocessing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'predict'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'NaN_processing' is not defined"
     ]
    }
   ],
   "source": [
    "# Predict\n",
    "test_dataset = pd.read_csv('data/test.csv')\n",
    "#test_dataset = NaN_check(test_dataset)\n",
    "test_dataset = NaN_processing(test_dataset)\n",
    "X_sub, y_sub, test_dataset = data_preprocessing(test_dataset, mode='predict')\n",
    "\n",
    "y_pred = rndm_regressor.predict(X_sub)\n",
    "\n",
    "output = pd.DataFrame({'Id': test_dataset['Id'], 'SalePrice': y_pred})\n",
    "output.to_csv('my_submission.csv', index=False)\n",
    "print(\"Your submission was successfully saved!\")"
   ]
  }
 ]
}